{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kaggle-environments -U > /dev/null 2>&1s\n!cp -r ../input/lux-ai-2021/* .","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:40.102401Z","iopub.execute_input":"2021-11-23T12:58:40.102621Z","iopub.status.idle":"2021-11-23T12:58:41.590754Z","shell.execute_reply.started":"2021-11-23T12:58:40.102590Z","shell.execute_reply":"2021-11-23T12:58:41.589808Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom pathlib import Path\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom itertools import count","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-23T12:58:41.593068Z","iopub.execute_input":"2021-11-23T12:58:41.593535Z","iopub.status.idle":"2021-11-23T12:58:41.600293Z","shell.execute_reply.started":"2021-11-23T12:58:41.593482Z","shell.execute_reply":"2021-11-23T12:58:41.599391Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from lux.constants import Constants\nfrom lux.game import Game\nfrom kaggle_environments import make","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:42.816934Z","iopub.execute_input":"2021-11-23T12:58:42.817473Z","iopub.status.idle":"2021-11-23T12:58:42.879488Z","shell.execute_reply.started":"2021-11-23T12:58:42.817440Z","shell.execute_reply":"2021-11-23T12:58:42.878730Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 0) Set Inputs","metadata":{}},{"cell_type":"code","source":"# Global Variables\n# < SYSTEM >\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAVE_PATH = './policy_network'\n\n# < TRAINING >\nEPS_START = 0.5\nEPS_END = 0.01\nEPS_DECAY = 200\nBATCH_SIZE = 32\nNUM_EPOCHS = 1000\nGAMMA = 0.999\nGAME_STEP = 0\nSTEPS_DONE = 0\nREPLAY_CAPACITY = 1000\nTARGET_UPDATE = 10\nLEARNING_RATE = 1e-2\nBEST_ACCURACY = 0\n\n# < MAP >\nN_ACTIONS = 5\nMAX_WIDTH = 32\nMAX_HEIGHT = 32\nWIDTH = 24\nHEIGHT = 24","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:43.395780Z","iopub.execute_input":"2021-11-23T12:58:43.396465Z","iopub.status.idle":"2021-11-23T12:58:43.453760Z","shell.execute_reply.started":"2021-11-23T12:58:43.396426Z","shell.execute_reply":"2021-11-23T12:58:43.452988Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"game_state = None\ndef get_game_state(observation):\n    global game_state\n    \n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation[\"player\"]\n    else:\n        game_state._update(observation[\"updates\"])\n    return game_state\n\n\ndef in_city(pos):    \n    try:\n        city = game_state.map.get_cell_by_pos(pos).citytile\n        return city is not None and city.team == game_state.id\n    except:\n        return False\n\n\ndef call_func(obj, method, args=[]):\n    return getattr(obj, method)(*args)\n\n\nunit_actions = [('move', 'n'), ('move', 's'), ('move', 'w'), ('move', 'e'), ('build_city',)]\ndef get_action(policy, unit, dest):    \n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * GAME_STEP / EPS_DECAY)\n    \n    for label in np.argsort(policy)[::-1]:\n        # Add Noise\n        debug = False\n        if sample > eps_threshold:\n            act = unit_actions[label]\n        else:\n            act = unit_actions[random.randrange(N_ACTIONS)]\n            debug = True\n        pos = unit.pos.translate(act[-1], 1) or unit.pos\n        \n        if pos not in dest or in_city(pos):\n            if debug:\n                print( f'[GET RAND ACTION] policy {label} at {pos}/[{dest}] - act:{act}' )\n            else :\n                print( f'[GET EXEC ACTION] policy {label} at {pos}/[{dest}] - act:{act}' )\n            return call_func(unit, *act), pos\n            \n    print( f'[GET ACTION] move center {label} at {pos}/[{dest}] - act:{act}' )\n    return unit.move('c'), unit.pos\n\n\ndef agent(observation, configuration):\n    global game_state, policyNet, targetNet\n    model = CLuxNet(N_ACTIONS)\n    game_state = get_game_state(observation)    \n    player = game_state.players[observation.player]\n    actions = []\n    \n    # City Actions\n    unit_count = len(player.units)\n    for city in player.cities.values():\n        for city_tile in city.citytiles:\n            if city_tile.can_act():\n                if unit_count < player.city_tile_count: \n                    actions.append(city_tile.build_worker())\n                    unit_count += 1\n                elif not player.researched_uranium():\n                    actions.append(city_tile.research())\n                    player.research_points += 1\n    \n    # Worker Actions\n    dest = []\n    model.load_state_dict(targetNet.state_dict())\n    for unit in player.units:\n        #print( \"agent call get_action:\", unit.id, \"-\", unit.can_act(), f'({unit.pos})', in_city(unit.pos) )\n        if unit.can_act() and (game_state.turn % 40 < 30 or not in_city(unit.pos)):\n            nStep: int = observation['step']\n            nXShift: int = (32 - observation['width']) // 2\n            nYShift: int = (32 - observation['height']) // 2\n            state = updateMap(nStep ,\\\n                              nXShift, \\\n                              nYShift, \\\n                              0, \\\n                              unit.id, \\\n                              observation['updates'])\n            with torch.no_grad():\n                p = model(torch.from_numpy(state).float().unsqueeze(0))\n\n            policy = p.squeeze(0).numpy()\n\n            action, pos = get_action(policy, unit, dest)\n            actions.append(action)\n            dest.append(pos)\n\n    return actions","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:43.595837Z","iopub.execute_input":"2021-11-23T12:58:43.596435Z","iopub.status.idle":"2021-11-23T12:58:43.618733Z","shell.execute_reply.started":"2021-11-23T12:58:43.596400Z","shell.execute_reply":"2021-11-23T12:58:43.617886Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"INPUT_CONSTANTS = Constants.INPUT_CONSTANTS\nRESOURCE_TYPES = Constants.RESOURCE_TYPES\n\ndef updateMap(nStep: int, \\\n              nXShift: int, \\\n              nYShift: int, \\\n              nTeam: int, \\\n              sUId: str, \\\n              updateList: list) -> list:\n\n    # indexing\n    # rp  - gameMap[0:2]                  #resource points\n    # r   - gameMap[2:5]                  #resource\n    # u   - gameMap[5:13]                 #unit\n    # c   - ...it only consumes fuels     #city\n    # ct  - gameMap[8:12]                 #citytile\n    # ccd - gameMap[]                     #roads (city cool down)\n\n    rpStart = 0\n    rStart = 2\n    uStart = 5\n    ctStart = 8\n\n    gameMap = np.zeros((20, MAX_WIDTH, MAX_HEIGHT))\n    cityDict: dict = {}\n\n    for update in updateList:\n        cmdList: list[str] = update.split(' ')\n\n        sIdentifier: str = cmdList[0]\n        if INPUT_CONSTANTS.RESEARCH_POINTS == sIdentifier:\n            team = int(cmdList[1])\n            rp = int(cmdList[2])\n            idx = rpStart + (team - nTeam) % 2\n            value = min(rp, 200) / 200\n            gameMap[idx, :] = value\n\n        elif INPUT_CONSTANTS.RESOURCES == sIdentifier:\n            rtype = cmdList[1]\n            x = int(cmdList[2]) + nXShift\n            y = int(cmdList[3]) + nYShift\n            amt = int(float(cmdList[4]))\n            idx = rStart + {'wood':0, 'coal':1, 'uranium':2}[rtype]\n            value = amt / 800\n            gameMap[idx, x, y] = value\n\n        elif INPUT_CONSTANTS.UNITS == sIdentifier:\n            utype = int(cmdList[1])\n            team = int(cmdList[2])\n            uid = cmdList[3]\n            x = int(cmdList[4])\n            y = int(cmdList[5])\n            cooldown = float(cmdList[6]) / 6.0\n            wood = int(cmdList[7])\n            coal = int(cmdList[8])\n            uranium = int(cmdList[9])\n            resources = (wood + coal + uranium) / 100\n\n            if sUId == uid:\n                idx = uStart\n                value = (1, resources)\n                gameMap[idx:idx+2, x, y] = value\n            else:\n                idx = uStart + 2\n                value = (1, cooldown, resources)\n                gameMap[idx:idx+3, x, y] = value\n\n        elif INPUT_CONSTANTS.CITY == sIdentifier:\n            team = int(cmdList[1])\n            cid: str = cmdList[2]\n            fuel = float(cmdList[3])\n            lightupkeep = float(cmdList[4])\n            cityDict[cid] = min(fuel / lightupkeep, 10) / 10\n\n        elif INPUT_CONSTANTS.CITY_TILES == sIdentifier:\n            team = int(cmdList[1])\n            cid: str = cmdList[2]\n            x = int(cmdList[3]) + nXShift\n            y = int(cmdList[4]) + nYShift\n            cooldown = float(cmdList[5])\n            idx = ctStart + (team - nTeam) % 2 * 2\n            value = (1, cityDict[cid])\n            gameMap[idx:idx+2, x, y] = value\n\n        elif INPUT_CONSTANTS.ROADS == sIdentifier:\n            x = int(cmdList[1])\n            y = int(cmdList[2])\n            road = float(cmdList[3])\n\n\n    # Day/Night Cycle\n    gameMap[17, :] = nStep % 40 / 40\n    # Turns\n    gameMap[18, :] = nStep / 360\n    # Map Size\n    gameMap[19, nXShift:MAX_WIDTH-nXShift, nYShift:MAX_HEIGHT-nYShift] = 1\n\n    return gameMap\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:43.932916Z","iopub.execute_input":"2021-11-23T12:58:43.933246Z","iopub.status.idle":"2021-11-23T12:58:43.951462Z","shell.execute_reply.started":"2021-11-23T12:58:43.933213Z","shell.execute_reply":"2021-11-23T12:58:43.950641Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def toLabel(player, action):\n    if action is None or len(action) < 1:\n        return f'u_{player}', None\n    strs = action[0].split(' ')\n    unit_id = strs[1]\n    if strs[0] == 'm':\n        label = {'c': None, 'n': 0, 's': 1, 'w': 2, 'e': 3}[strs[2]]\n    elif strs[0] == 'bcity':\n        label = 4\n    else:\n        label = None\n    return unit_id, label\n\ndef depletedResources(obs):\n    for u in obs['updates']:\n        if u.split(' ')[0] == 'r':\n            return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:44.166385Z","iopub.execute_input":"2021-11-23T12:58:44.166864Z","iopub.status.idle":"2021-11-23T12:58:44.173654Z","shell.execute_reply.started":"2021-11-23T12:58:44.166829Z","shell.execute_reply":"2021-11-23T12:58:44.172655Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 1) Network","metadata":{}},{"cell_type":"code","source":"class CBasicConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            input_dim, output_dim, \n            kernel_size=kernel_size, \n            padding=(kernel_size[0] // 2, kernel_size[1] // 2)\n        )\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = self.conv(x)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\nclass CLuxNet(nn.Module):\n    def __init__( self, nActions ):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv = CBasicConv2d(20, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([\n            CBasicConv2d(filters, filters, (3, 3), True) for _ in range( layers )\n        ])\n        self.head = nn.Linear( filters, nActions, bias=False )\n\n    def forward(self, x):\n        h = F.relu_( self.conv(x) )\n        for b in self.blocks:\n            h = F.relu_( h + b( h ) )\n        h = ( h * x[:, :1] ).view( h.size(0), h.size(1), -1 ).sum(-1)\n        ret = self.head( h )\n        return ret","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:44.633082Z","iopub.execute_input":"2021-11-23T12:58:44.633838Z","iopub.status.idle":"2021-11-23T12:58:44.644533Z","shell.execute_reply.started":"2021-11-23T12:58:44.633793Z","shell.execute_reply":"2021-11-23T12:58:44.643638Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 2) Replay Memory","metadata":{}},{"cell_type":"code","source":"# Input for ReplayMemory\nfrom collections import namedtuple, deque\nData = namedtuple('Data',\n                  ('state', 'action', 'next_state', 'reward'))\n\n# state: list(str) = state\n# action: list(str) = step[0]['action']\n# next_state: list(str) = step[0]['observation']['updates']\nclass CReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([],maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a Data\"\"\"\n        self.memory.append(Data(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n    \nmemory = CReplayMemory(REPLAY_CAPACITY)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:45.279518Z","iopub.execute_input":"2021-11-23T12:58:45.280047Z","iopub.status.idle":"2021-11-23T12:58:45.287098Z","shell.execute_reply.started":"2021-11-23T12:58:45.280012Z","shell.execute_reply":"2021-11-23T12:58:45.286168Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 3) Select Action w/ noise","metadata":{}},{"cell_type":"markdown","source":"### 4) Optimize Model","metadata":{}},{"cell_type":"code","source":"def optimizeModel( memory: CReplayMemory, \\\n                   policyNet: CLuxNet, \\\n                   targetNet: CLuxNet, \\\n                   optimizer ) -> None:\n    global BEST_ACCURACY, GAME_STEP\n    # -1) return exceptions\n    if STEPS_DONE == 0 or STEPS_DONE % BATCH_SIZE != 0 or len(memory) < BATCH_SIZE:\n        return\n    \n    GAME_STEP += 1\n    \n    # 1) fetch memory in batch size\n    datas = memory.sample(BATCH_SIZE)\n\n    # 2) make in a bulk list of Data type\n    datas = Data(*zip(*datas))\n    \n    # 3) concatenate state, action, reward\n    mask = torch.tensor(\n        tuple(map(lambda a: a is not None, datas.action)),\\\n        device=DEVICE,\\\n        dtype=torch.bool\n    )\n    \n    # 어떻게 move c를 None으로 두고 아래 코드를 동작하게 할 수 있을까? - Check\n    states = torch.cat([torch.tensor(s) for s in datas.state])\n    actions = torch.cat([torch.tensor(a) for a in np.array(datas.action).reshape(1, -1)])\n    rewards = torch.cat([torch.tensor(r) for r in np.array(datas.reward)])\n    \n    # 4) next state mask\n    nextStateMask = torch.tensor(\n        tuple(map(lambda n_s: n_s is not None, datas.next_state)), \\\n        device = DEVICE, \\\n        dtype = torch.bool\n    )\n    \n    # 5) concatenate next state\n    nextStates = torch.cat([\n        torch.tensor(n_s) for n_s in datas.next_state if n_s is not None\n    ])\n    \n    # ** RESIZE INPUTS & SELECT DEVICE\n    states = states.view(BATCH_SIZE, -1, MAX_WIDTH, MAX_HEIGHT).float().to(DEVICE)\n    nextStates = nextStates.view(BATCH_SIZE, -1, MAX_WIDTH, MAX_HEIGHT).float().to(DEVICE)    \n    actions = actions.view(1, -1).to(DEVICE)\n    rewards.to(DEVICE)\n    \n    # 6) Compute Q-Value( Q(s_t, a) ), and select the columns of actions taken for each batch size\n    pred = policyNet(states)\n    qValue = policyNet(states).gather(1, actions)\n    \n    # 7) Compute V(s_{t+1}) for all next states\n    vValue = torch.zeros(BATCH_SIZE, device=DEVICE)\n    vValueIdx = targetNet(nextStates).argmax(1).detach()\n    vValue[nextStateMask] = targetNet(nextStates).max(1)[0].detach() #select action 가능\n    \n    # 8) Compute expected Q-Values with discount rate\n    expcQValue = (vValue * GAMMA + rewards).unsqueeze(1)\n    \n    # 9) Compute Huber Loss\n    # criterion = nn.SmoothL1Loss()\n    criterion = nn.CrossEntropyLoss()\n    loss = criterion(pred, actions.data.squeeze(0)[nextStateMask])\n    print( \"-------- PREDICTION ----------\\n\", vValue[nextStateMask], \"\\n\", vValueIdx, \"\\n\", actions.data.squeeze(0)[nextStateMask] )\n    acc = torch.sum(vValueIdx[nextStateMask] == actions.data.squeeze(0)[nextStateMask]) / len(vValueIdx[nextStateMask])\n    \n    # 10) Save the best model\n    if BEST_ACCURACY < acc:\n        BEST_ACCURACY = acc\n        torch.save(targetNet, SAVE_PATH)\n        \n    # 11) Log\n    print(f':: {STEPS_DONE} STEP :: Acc({acc}/{BEST_ACCURACY}), loss({loss}), QValue({expcQValue.min()}, {expcQValue.max()})')\n    \n    # 12) Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policyNet.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:46.386216Z","iopub.execute_input":"2021-11-23T12:58:46.386804Z","iopub.status.idle":"2021-11-23T12:58:46.404176Z","shell.execute_reply.started":"2021-11-23T12:58:46.386765Z","shell.execute_reply":"2021-11-23T12:58:46.403164Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 5) Trainig","metadata":{}},{"cell_type":"code","source":"LOG_LEVEL = 1 # 1 - ERROR / 2 - WARN / 3 - INFO\ndef trainModel(policyNet: CLuxNet, \\\n               targetNet: CLuxNet, \\\n               optimizer, \\\n               width: int, \\\n               height: int) -> None:\n    global STEPS_DONE\n        \n    targetNet.cuda()\n    policyNet.cuda()\n    \n    targetNet.eval()\n    policyNet.train()\n        \n    for epoch in range(NUM_EPOCHS):\n        env = make(\"lux_ai_2021\", configuration={\"width\": WIDTH, \"height\": HEIGHT, \"loglevel\": LOG_LEVEL, \"annotations\": True}, debug=True)\n        steps = env.run([agent, agent])\n        \n        xShift, yShift = 0, 0\n        nextState = torch.zeros((20, MAX_WIDTH, MAX_HEIGHT), dtype=torch.float32)\n        for s in steps:\n            if len(s) > 1:\n                step = s[0]\n            else:\n                step = s\n            \n            observation = step['observation']\n            \n            depletedResources(observation)\n            \n            nStep: int = observation['step']\n            nTeam: int = observation['player']\n            sUId, action = toLabel(nTeam, step['action'])\n            if action is None:\n                continue\n            \n            if nStep == 0:\n                width, height = observation['width'], observation['height']\n                xShift, yShift = (MAX_WIDTH - width) // 2, (MAX_HEIGHT - height) // 2\n                    \n            state = nextState\n            reward = torch.tensor([step['reward'] if step['reward'] is not None else 0], device=DEVICE)\n            nextState = updateMap(nStep, xShift, yShift, nTeam, sUId, observation['updates'])\n            \n            memory.push(state, action, nextState, reward)\n            optimizeModel(memory, policyNet, targetNet, optimizer)\n            \n            STEPS_DONE += 1\n        \n        if epoch % TARGET_UPDATE == 0:\n            targetNet.load_state_dict(policyNet.state_dict())\n            print(\":: NETWORK UPDATED ::\")\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:46.932738Z","iopub.execute_input":"2021-11-23T12:58:46.933147Z","iopub.status.idle":"2021-11-23T12:58:46.945740Z","shell.execute_reply.started":"2021-11-23T12:58:46.933113Z","shell.execute_reply":"2021-11-23T12:58:46.944896Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 6) Run","metadata":{}},{"cell_type":"code","source":"policyNet = CLuxNet(N_ACTIONS).to(DEVICE)\ntargetNet = CLuxNet(N_ACTIONS).to(DEVICE)\ntargetNet.load_state_dict(policyNet.state_dict())\noptimizer = torch.optim.AdamW(policyNet.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:47.771029Z","iopub.execute_input":"2021-11-23T12:58:47.771570Z","iopub.status.idle":"2021-11-23T12:58:53.288867Z","shell.execute_reply.started":"2021-11-23T12:58:47.771537Z","shell.execute_reply":"2021-11-23T12:58:53.287963Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainModel(policyNet, targetNet, optimizer, WIDTH, HEIGHT)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:58:53.290596Z","iopub.execute_input":"2021-11-23T12:58:53.290847Z","iopub.status.idle":"2021-11-23T12:59:33.107588Z","shell.execute_reply.started":"2021-11-23T12:58:53.290812Z","shell.execute_reply":"2021-11-23T12:59:33.106408Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#print(*policyNet.parameters())\nfor i in policyNet.parameters():\n    print( i.shape, i[0][2] )\n    break\n\nfor i in targetNet.parameters():\n    print( i.shape, i[0][2] )\n    break","metadata":{"execution":{"iopub.status.busy":"2021-11-23T11:38:01.311952Z","iopub.status.idle":"2021-11-23T11:38:01.312898Z","shell.execute_reply.started":"2021-11-23T11:38:01.31253Z","shell.execute_reply":"2021-11-23T11:38:01.312562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 24, \"height\": 24, \"loglevel\": 2, \"annotations\": True}, debug=False)\nsteps = env.run([agent, agent])\n#env.render(mode=\"ipython\", width=1200, height=800)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:28:20.107524Z","iopub.execute_input":"2021-11-18T13:28:20.107799Z","iopub.status.idle":"2021-11-18T13:28:20.143992Z","shell.execute_reply.started":"2021-11-18T13:28:20.10777Z","shell.execute_reply":"2021-11-18T13:28:20.142981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"execution":{"iopub.status.busy":"2021-11-14T14:10:40.905879Z","iopub.status.idle":"2021-11-14T14:10:40.907332Z","shell.execute_reply.started":"2021-11-14T14:10:40.90698Z","shell.execute_reply":"2021-11-14T14:10:40.907008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainModel(policyNet, targetNet, optimizer, WIDTH, HEIGHT)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T14:10:40.908779Z","iopub.status.idle":"2021-11-14T14:10:40.909723Z","shell.execute_reply.started":"2021-11-14T14:10:40.909421Z","shell.execute_reply":"2021-11-14T14:10:40.90945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
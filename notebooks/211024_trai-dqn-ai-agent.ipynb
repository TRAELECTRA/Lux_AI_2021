{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kaggle-environments -U > /dev/null 2>&1\n!cp -r ../input/lux-ai-2021/* .","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:27:44.111003Z","iopub.execute_input":"2021-10-24T08:27:44.111578Z","iopub.status.idle":"2021-10-24T08:27:52.612075Z","shell.execute_reply.started":"2021-10-24T08:27:44.111542Z","shell.execute_reply":"2021-10-24T08:27:52.611159Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom pathlib import Path\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-24T08:27:42.439075Z","iopub.execute_input":"2021-10-24T08:27:42.439576Z","iopub.status.idle":"2021-10-24T08:27:43.366397Z","shell.execute_reply.started":"2021-10-24T08:27:42.439540Z","shell.execute_reply":"2021-10-24T08:27:43.365691Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Run Episode","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 24, \"height\": 24, \"loglevel\": 2, \"annotations\": True}, debug=False)\nsteps = env.run(['agent.py', 'agent.py'])\n# env.render(mode=\"ipython\", width=1200, height=800)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Inputs","metadata":{}},{"cell_type":"code","source":"# Global Variables\n# < MAP >\nwidth, height = 0, 0","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:16:11.839820Z","iopub.execute_input":"2021-10-24T08:16:11.840403Z","iopub.status.idle":"2021-10-24T08:16:11.844991Z","shell.execute_reply.started":"2021-10-24T08:16:11.840367Z","shell.execute_reply":"2021-10-24T08:16:11.844216Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Input for ReplayMemory\nfrom collections import namedtuple\nData = namedtuple('Data',\n                  ('state', 'action', 'next_state', 'reward'))\n\n# state: list(str) = state\n# action: list(str) = step[0]['action']\n# next_state: list(str) = step[0]['observation']['updates']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"step[0] = [{'action': [], 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 60, 'step': 0, 'width': 24, 'height': 24, 'reward': 0, 'globalUnitIDCount': 2, 'globalCityIDCount': 2, 'player': 0, 'updates': ['0', '24 24', 'rp 0 0', 'rp 1 0', 'r uranium 0 0 310', 'r uranium 0 14 344', 'r uranium 0 23 304', 'r wood 3 10 333', 'r coal 3 23 407', 'r wood 4 7 323', 'r wood 4 8 324', 'r wood 4 10 318', 'r wood 4 11 360', 'r wood 5 8 345', 'r wood 5 9 303', 'r wood 5 10 351', 'r wood 5 21 800', 'r wood 5 22 800', 'r wood 6 8 387', 'r wood 6 9 342', 'r wood 6 10 326', 'r wood 6 11 398', 'r wood 6 12 393', 'r wood 6 19 350', 'r wood 6 20 331', 'r wood 6 21 800', 'r wood 7 10 355', 'r wood 7 11 376', 'r wood 7 12 310', 'r wood 7 19 364', 'r wood 7 20 396', 'r wood 8 11 346', 'r wood 8 12 394', 'r wood 8 20 385', 'r wood 9 10 376', 'r uranium 10 0 324', 'r uranium 10 1 322', 'r uranium 11 0 349', 'r coal 11 5 391', 'r coal 11 6 418', 'r uranium 12 0 349', 'r coal 12 5 391', 'r coal 12 6 418', 'r uranium 13 0 324', 'r uranium 13 1 322', 'r wood 14 10 376', 'r wood 15 11 346', 'r wood 15 12 394', 'r wood 15 20 385', 'r wood 16 10 355', 'r wood 16 11 376', 'r wood 16 12 310', 'r wood 16 19 364', 'r wood 16 20 396', 'r wood 17 8 387', 'r wood 17 9 342', 'r wood 17 10 326', 'r wood 17 11 398', 'r wood 17 12 393', 'r wood 17 19 350', 'r wood 17 20 331', 'r wood 17 21 800', 'r wood 18 8 345', 'r wood 18 9 303', 'r wood 18 10 351', 'r wood 18 21 800', 'r wood 18 22 800', 'r wood 19 7 323', 'r wood 19 8 324', 'r wood 19 10 318', 'r wood 19 11 360', 'r wood 20 10 333', 'r coal 20 23 407', 'r uranium 23 0 310', 'r uranium 23 14 344', 'r uranium 23 23 304', 'u 0 0 u_1 6 22 0 0 0 0', 'u 0 1 u_2 17 22 0 0 0 0', 'c 0 c_1 0 23', 'c 1 c_2 0 23', 'ct 0 c_1 6 22 0', 'ct 1 c_2 17 22 0', 'ccd 6 22 6', 'ccd 17 22 6', 'D_DONE']}, 'status': 'ACTIVE'}, {'action': [], 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 60, 'reward': 0, 'player': 1}, 'status': 'ACTIVE'}]\n\nstep[1] = [{'action': None, 'reward': None, 'info': {}, 'observation': {'remainingOverageTime': 60, 'step': 1, 'width': 24, 'height': 24, 'reward': 10001, 'globalUnitIDCount': 2, 'globalCityIDCount': 2, 'player': 0, 'updates': ['rp 0 0', 'rp 1 0', 'r uranium 0 0 310', 'r uranium 0 14 344', 'r uranium 0 23 304', 'r wood 3 10 342', 'r coal 3 23 407', 'r wood 4 7 332', 'r wood 4 8 333', 'r wood 4 10 326', 'r wood 4 11 369', 'r wood 5 8 354', 'r wood 5 9 311', 'r wood 5 10 360', 'r wood 5 21 800', 'r wood 5 22 780', 'r wood 6 8 397', 'r wood 6 9 351', 'r wood 6 10 335', 'r wood 6 11 408', 'r wood 6 12 403', 'r wood 6 19 359', 'r wood 6 20 340', 'r wood 6 21 780', 'r wood 7 10 364', 'r wood 7 11 386', 'r wood 7 12 318', 'r wood 7 19 374', 'r wood 7 20 406', 'r wood 8 11 355', 'r wood 8 12 404', 'r wood 8 20 395', 'r wood 9 10 386', 'r uranium 10 0 324', 'r uranium 10 1 322', 'r uranium 11 0 349', 'r coal 11 5 391', 'r coal 11 6 418', 'r uranium 12 0 349', 'r coal 12 5 391', 'r coal 12 6 418', 'r uranium 13 0 324', 'r uranium 13 1 322', 'r wood 14 10 386', 'r wood 15 11 355', 'r wood 15 12 404', 'r wood 15 20 395', 'r wood 16 10 364', 'r wood 16 11 386', 'r wood 16 12 318', 'r wood 16 19 374', 'r wood 16 20 406', 'r wood 17 8 397', 'r wood 17 9 351', 'r wood 17 10 335', 'r wood 17 11 408', 'r wood 17 12 403', 'r wood 17 19 359', 'r wood 17 20 340', 'r wood 17 21 780', 'r wood 18 8 354', 'r wood 18 9 311', 'r wood 18 10 360', 'r wood 18 21 800', 'r wood 18 22 780', 'r wood 19 7 332', 'r wood 19 8 333', 'r wood 19 10 326', 'r wood 19 11 369', 'r wood 20 10 342', 'r coal 20 23 407', 'r uranium 23 0 310', 'r uranium 23 14 344', 'r uranium 23 23 304', 'u 0 0 u_1 6 22 0 0 0 0', 'u 0 1 u_2 17 22 0 0 0 0', 'c 0 c_1 40 23', 'c 1 c_2 40 23', 'ct 0 c_1 6 22 0', 'ct 1 c_2 17 22 0', 'ccd 6 22 6', 'ccd 17 22 6', 'D_DONE']}, 'status': 'ERROR'}, {'action': None, 'reward': None, 'info': {}, 'observation': {'remainingOverageTime': 60, 'reward': 10001, 'player': 1}, 'status': 'ERROR'}]\n\nstep[100] = {'action': ['m u_1 w'],\n 'reward': 10001,\n 'info': {},\n 'observation': {'remainingOverageTime': 60,\n  'step': 100,\n  'width': 24,\n  'height': 24,\n  'reward': 10001,\n  'globalUnitIDCount': 2,\n  'globalCityIDCount': 2,\n  'player': 0,\n  'updates': ['rp 0 0',\n   'rp 1 0',\n   'r coal 0 3 367',\n   'r coal 0 4 389',\n   'r coal 0 8 410',\n   'r uranium 0 23 302',\n   'r wood 1 21 500',\n   'r wood 1 22 500',\n   'r wood 1 23 500',\n   'r wood 2 21 500',\n   'r wood 2 22 500',\n   'r wood 3 22 500',\n   'r wood 6 11 500',\n   'r wood 7 10 500',\n   'r wood 8 1 500',\n   'r wood 8 2 500',\n   'r wood 8 3 500',\n   'r wood 8 9 500',\n   'r wood 8 10 500',\n   'r uranium 8 16 328',\n   'r uranium 8 17 310',\n   'r coal 8 22 354',\n   'r coal 8 23 396',\n   'r wood 9 9 500',\n   'r wood 9 10 500',\n   'r uranium 9 17 342',\n   'r coal 9 23 394',\n   'r wood 10 10 500',\n   'r wood 13 10 500',\n   'r wood 14 9 500',\n   'r wood 14 10 500',\n   'r uranium 14 17 342',\n   'r coal 14 23 394',\n   'r wood 15 1 500',\n   'r wood 15 2 500',\n   'r wood 15 3 500',\n   'r wood 15 9 500',\n   'r wood 15 10 500',\n   'r uranium 15 16 328',\n   'r uranium 15 17 310',\n   'r coal 15 22 354',\n   'r coal 15 23 396',\n   'r wood 16 10 500',\n   'r wood 17 11 500',\n   'r wood 20 22 500',\n   'r wood 21 21 500',\n   'r wood 21 22 500',\n   'r wood 22 21 500',\n   'r wood 22 22 500',\n   'r wood 22 23 500',\n   'r coal 23 3 367',\n   'r coal 23 4 389',\n   'r coal 23 8 410',\n   'r uranium 23 23 302',\n   'u 0 0 u_1 7 2 0 0 0 0',\n   'u 0 1 u_2 16 2 0 0 0 0',\n   'c 0 c_1 2240 23',\n   'c 1 c_2 2240 23',\n   'ct 0 c_1 7 2 0',\n   'ct 1 c_2 16 2 0',\n   'ccd 7 2 6',\n   'ccd 16 2 6',\n   'D_DONE']},\n 'status': 'ACTIVE'}","metadata":{}},{"cell_type":"code","source":"# input for optimize_model\n\n# return map states = [][]\ndef makeInputMap(updateList:list) -> list:\n    global width, height\n    nXShift:int = (32 - width) // 2\n    nYShift:int = (32 - height) // 2\n        \n    # 20 = Position&Cargo(:2) + \n    gameMapList: np.ndarray(np.float32) = np.zeros()\n        \n    dRes = dict()\n    player = lStep[0]\n    \n    sAction = player['action']\n    if sAction[3] == 'u':\n        \n    dRes['action']: list(str) = player['action']\n    dRes['reward']: int = player['reward']\n    dRes['']\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Replay Memory","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select Action","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimize Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainig","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:27:52.614215Z","iopub.execute_input":"2021-10-24T08:27:52.614668Z","iopub.status.idle":"2021-10-24T08:27:52.663513Z","shell.execute_reply.started":"2021-10-24T08:27:52.614627Z","shell.execute_reply":"2021-10-24T08:27:52.662851Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def to_label(action):\n    strs = action.split(' ')\n    unit_id = strs[1]\n    if strs[0] == 'm':\n        label = {'c': None, 'n': 0, 's': 1, 'w': 2, 'e': 3}[strs[2]]\n    elif strs[0] == 'bcity':\n        label = 4\n    else:\n        label = None\n    return unit_id, label\n\n\ndef depleted_resources(obs):\n    for u in obs['updates']:\n        if u.split(' ')[0] == 'r':\n            return False\n    return True\n\n\ndef create_dataset_from_json(episode_dir, team_name='Toad Brigade'): \n    obses = {}\n    samples = []\n    append = samples.append\n    \n    episodes = [path for path in Path(episode_dir).glob('*.json') if 'output' not in path.name]\n    for filepath in tqdm(episodes): \n        with open(filepath) as f:\n            json_load = json.load(f)\n\n        ep_id = json_load['info']['EpisodeId']\n        index = np.argmax([r or 0 for r in json_load['rewards']])\n        if json_load['info']['TeamNames'][index] != team_name:\n            continue\n\n        for i in range(len(json_load['steps'])-1):\n            if json_load['steps'][i][index]['status'] == 'ACTIVE':\n                actions = json_load['steps'][i+1][index]['action']\n                obs = json_load['steps'][i][0]['observation']\n                \n                #Debugging\n                #print(\"obs:\", obs)\n                \n                if depleted_resources(obs):\n                    break\n                \n                obs['player'] = index\n                obs = dict([\n                    (k,v) for k,v in obs.items() \n                    if k in ['step', 'updates', 'player', 'width', 'height']\n                ])\n                obs_id = f'{ep_id}_{i}'\n                obses[obs_id] = obs\n                                \n                for action in actions:\n                    unit_id, label = to_label(action)\n                    if label is not None:\n                        append((obs_id, unit_id, label))\n\n    return obses, samples","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:27:53.958061Z","iopub.execute_input":"2021-10-24T08:27:53.958407Z","iopub.status.idle":"2021-10-24T08:27:53.980979Z","shell.execute_reply.started":"2021-10-24T08:27:53.958364Z","shell.execute_reply":"2021-10-24T08:27:53.980145Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"episode_dir = '../input/lux-ai-episodes'\nobses, samples = create_dataset_from_json(episode_dir)\n# print('obses:', len(obses), list(obses.items())[32574], '\\nsamples:', len(samples), samples[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:01.462357Z","iopub.execute_input":"2021-10-24T08:28:01.462719Z","iopub.status.idle":"2021-10-24T08:28:05.538330Z","shell.execute_reply.started":"2021-10-24T08:28:01.462683Z","shell.execute_reply":"2021-10-24T08:28:05.537498Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"'26814974_0', { 'height': 12,  \n                'width': 12,  \n                'player': 0,   \n                'step': 0,   \n                'updates': ['0',  \n                            '12 12',  \n                            'rp 0 0',   \n                            'rp 1 0',   \n                            'r uranium 0 0 326',   \n                            'r wood 0 5 800',   \n                            'r wood 0 6 800',   \n                            'r coal 0 10 386',   \n                            'r coal 0 11 366',   \n                            'r wood 1 5 800',   \n                            'r wood 4 2 371',    \n                            'r wood 4 3 340',   \n                            'r wood 5 1 397',   \n                            'r wood 5 2 344',   \n                            'r wood 5 3 326',   \n                            'r wood 5 10 377',   \n                            'r wood 5 11 390',   \n                            'r wood 6 1 397',   \n                            'r wood 6 2 344',   \n                            'r wood 6 3 326',   \n                            'r wood 6 10 377',   \n                            'r wood 6 11 390',   \n                            'r wood 7 2 371',   \n                            'r wood 7 3 340',   \n                            'r wood 10 5 800',   \n                            'r uranium 11 0 326',   \n                            'r wood 11 5 800',   \n                            'r wood 11 6 800',   \n                            'r coal 11 10 386',   \n                            'r coal 11 11 366',   \n                            'u 0 0 u_1 1 6 0 0 0 0',   \n                            'u 0 1 u_2 10 6 0 0 0 0',   \n                            'c 0 c_1 0 23',   \n                            'c 1 c_2 0 23',   \n                            'ct 0 c_1 1 6 0',   \n                            'ct 1 c_2 10 6 0',   \n                            'ccd 1 6 6',   \n                            'ccd 10 6 6',   \n                            'D_DONE']}\nsamples[0] ==> ('26814974_0', 'u_1', 0)\n                            \n'26691692_313', {'height': 12, 'player': 0, 'step': 313, 'updates': ['rp 0 190', 'rp 1 131', 'r uranium 0 0 322', 'r uranium 0 1 311', 'r uranium 0 11 304', 'r uranium 11 0 322', 'r uranium 11 1 311', 'r uranium 11 11 304', 'u 0 0 u_3 6 10 0 0 0 0', 'u 0 0 u_7 7 1 0 0 0 0', 'u 0 0 u_15 5 10 0 0 0 0', 'u 0 0 u_26 5 1 0 0 0 0', 'u 0 0 u_28 6 10 0 0 0 0', 'u 0 0 u_29 4 10 0 0 0 0', 'u 0 0 u_32 4 9 0 0 0 0', 'u 0 0 u_35 4 11 0 0 0 0', 'u 0 0 u_37 6 1 0 0 0 0', 'u 0 0 u_41 7 10 0 0 0 0', 'u 0 1 u_2 5 9 0 0 0 0', 'u 0 1 u_8 5 9 0 0 0 0', 'u 0 1 u_14 5 9 0 0 0 0', 'u 0 1 u_18 5 9 0 0 0 0', 'u 0 1 u_24 5 9 0 0 0 0', 'c 0 c_19 364 62', 'c 1 c_30 17 36', 'c 0 c_31 3616 88', 'ct 0 c_19 5 1 0', 'ct 0 c_19 6 1 0', 'ct 0 c_19 5 0 9', 'ct 0 c_19 7 1 5', 'ct 1 c_30 5 9 9', 'ct 1 c_30 6 9 9', 'ct 0 c_31 7 10 8', 'ct 0 c_31 6 10 0', 'ct 0 c_31 4 10 0', 'ct 0 c_31 4 9 0', 'ct 0 c_31 4 11 6', 'ct 0 c_31 5 10 0', 'ccd 5 0 6', 'ccd 5 1 6', 'ccd 6 1 6', 'ccd 7 1 6', 'ccd 4 9 6', 'ccd 5 9 6', 'ccd 6 9 6', 'ccd 4 10 6', 'ccd 5 10 6', 'ccd 6 10 6', 'ccd 7 10 6', 'ccd 4 11 6', 'D_DONE'], 'width': 12}\n","metadata":{}},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2021-10-24T07:07:01.026813Z","iopub.execute_input":"2021-10-24T07:07:01.027286Z","iopub.status.idle":"2021-10-24T07:07:01.036794Z","shell.execute_reply.started":"2021-10-24T07:07:01.027244Z","shell.execute_reply":"2021-10-24T07:07:01.035902Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlabels = [sample[-1] for sample in samples]\nactions = ['north', 'south', 'west', 'east', 'bcity']\nn_actions = 5\nfor value, count in zip(*np.unique(labels, return_counts=True)):\n    print(f'{actions[value]:^5}: {count:>3}')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T07:07:01.038187Z","iopub.execute_input":"2021-10-24T07:07:01.038732Z","iopub.status.idle":"2021-10-24T07:07:01.083686Z","shell.execute_reply.started":"2021-10-24T07:07:01.038693Z","shell.execute_reply":"2021-10-24T07:07:01.082833Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Input for Neural Network\ndef make_input(obs, unit_id):\n    width, height = obs['width'], obs['height']\n    x_shift = (32 - width) // 2\n    y_shift = (32 - height) // 2\n    cities = {}\n    \n    b = np.zeros((20, 32, 32), dtype=np.float32)\n    \n    for update in obs['updates']:\n        strs = update.split(' ')\n        input_identifier = strs[0]\n        \n        if input_identifier == 'u':\n            x = int(strs[4]) + x_shift\n            y = int(strs[5]) + y_shift\n            wood = int(strs[7])\n            coal = int(strs[8])\n            uranium = int(strs[9])\n            if unit_id == strs[3]:\n                # Position and Cargo\n                b[:2, x, y] = (\n                    1,\n                    (wood + coal + uranium) / 100\n                )\n            else:\n                # Units\n                team = int(strs[2])\n                cooldown = float(strs[6])\n                idx = 2 + (team - obs['player']) % 2 * 3\n                b[idx:idx + 3, x, y] = (\n                    1,\n                    cooldown / 6,\n                    (wood + coal + uranium) / 100\n                )\n        elif input_identifier == 'ct':\n            # CityTiles\n            team = int(strs[1])\n            city_id = strs[2]\n            x = int(strs[3]) + x_shift\n            y = int(strs[4]) + y_shift\n            idx = 8 + (team - obs['player']) % 2 * 2\n            b[idx:idx + 2, x, y] = (\n                1,\n                cities[city_id]\n            )\n        elif input_identifier == 'r':\n            # Resources\n            r_type = strs[1]\n            x = int(strs[2]) + x_shift\n            y = int(strs[3]) + y_shift\n            amt = int(float(strs[4]))\n            b[{'wood': 12, 'coal': 13, 'uranium': 14}[r_type], x, y] = amt / 800\n        elif input_identifier == 'rp':\n            # Research Points\n            team = int(strs[1])\n            rp = int(strs[2])\n            b[15 + (team - obs['player']) % 2, :] = min(rp, 200) / 200\n        elif input_identifier == 'c':\n            # Cities\n            city_id = strs[2]\n            fuel = float(strs[3])\n            lightupkeep = float(strs[4])\n            cities[city_id] = min(fuel / lightupkeep, 10) / 10\n    \n    # Day/Night Cycle\n    b[17, :] = obs['step'] % 40 / 40\n    # Turns\n    b[18, :] = obs['step'] / 360\n    # Map Size\n    b[19, x_shift:32 - x_shift, y_shift:32 - y_shift] = 1\n\n    return b\n\n\nclass LuxDataset(Dataset):\n    def __init__(self, obses, samples):\n        self.obses = obses\n        self.samples = samples\n        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        obs_id, unit_id, action = self.samples[idx]\n        obs = self.obses[obs_id]\n        state = make_input(obs, unit_id)\n        \n        return state, action","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:14.046382Z","iopub.execute_input":"2021-10-24T08:28:14.047045Z","iopub.status.idle":"2021-10-24T08:28:14.066793Z","shell.execute_reply.started":"2021-10-24T08:28:14.046991Z","shell.execute_reply":"2021-10-24T08:28:14.065721Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class CBasicConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            input_dim, output_dim, \n            kernel_size=kernel_size, \n            padding=(kernel_size[0] // 2, kernel_size[1] // 2)\n        )\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = self.conv(x)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\nclass LuxNet(nn.Module):\n    def __init__( self, n_actions ):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv = CBasicConv2d(20, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([\n            CBasicConv2d(filters, filters, (3, 3), True) for _ in range( layers )\n        ])\n        self.head = nn.Linear( filters, n_actions, bias=False )\n\n    def forward(self, x):\n        h = F.relu_( self.conv(x) )\n        for b in self.blocks:\n            h = F.relu_( h + b( h ) )\n        h = ( h * x[:, :1] ).view( h.size(0), h.size(1), -1 ).sum(-1)\n        ret = self.head( h )\n        return ret ","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:14.330988Z","iopub.execute_input":"2021-10-24T08:28:14.331284Z","iopub.status.idle":"2021-10-24T08:28:14.342030Z","shell.execute_reply.started":"2021-10-24T08:28:14.331254Z","shell.execute_reply":"2021-10-24T08:28:14.340907Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"EPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 200\nsteps_done = 0\ndef select_action(state, model):\n    global steps_done, debug\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample > eps_threshold:\n        with torch.no_grad():\n            # t.max(1) will return largest column value of each row.\n            # second column on max result is index of where max element was\n            # found, so we pick action with the larger expected reward.\n            return policy_net(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:14.747825Z","iopub.execute_input":"2021-10-24T08:28:14.748041Z","iopub.status.idle":"2021-10-24T08:28:14.754459Z","shell.execute_reply.started":"2021-10-24T08:28:14.748017Z","shell.execute_reply":"2021-10-24T08:28:14.753694Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nTransition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([],maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:15.804641Z","iopub.execute_input":"2021-10-24T08:28:15.805068Z","iopub.status.idle":"2021-10-24T08:28:15.814003Z","shell.execute_reply.started":"2021-10-24T08:28:15.805024Z","shell.execute_reply":"2021-10-24T08:28:15.812937Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def optimize_model():\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n\n    # Compute a mask of non-final states and concatenate the batch elements\n    # (a final state would've been the one after which simulation ended)\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.bool)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n\n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # Compute Huber loss\n    criterion = nn.SmoothL1Loss()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:17.652759Z","iopub.execute_input":"2021-10-24T08:28:17.653032Z","iopub.status.idle":"2021-10-24T08:28:17.662295Z","shell.execute_reply.started":"2021-10-24T08:28:17.653002Z","shell.execute_reply":"2021-10-24T08:28:17.661654Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs):\n    global policy_net\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        model.cuda()\n        \n        dataloader = dataloaders_dict[phase]\n        for item in tqdm(dataloader, leave=False):\n            states = item[0].cuda().float()\n            actions = item[1].cuda().long()\n            \n            optimizer.zero_grad()\n            \n            counts = len(states)\n            for i in range(counts):\n                action = select_action(states[i], model)\n                _, reward, done, _ = env.step(action.item())\n                reward = torch.tensor([reward], device=device)\n                \n                \n                \n                \n            with torch.set_grad_enabled(phase == 'train'):\n                policy = select_action(states)\n            \n        for t in count():\n            # Select and perform an action\n            action = select_action(state)\n            _, reward, done, _ = env.step(action.item())\n            reward = torch.tensor([reward], device=device)\n\n            # Observe new state\n            last_screen = current_screen\n            current_screen = get_screen()\n            if not done:\n                next_state = current_screen - last_screen\n            else:\n                next_state = None\n\n            # Store the transition in memory\n            memory.push(state, action, next_state, reward)\n\n            # Move to the next state\n            state = next_state\n\n            # Perform one step of the optimization (on the policy network)\n            optimize_model()\n            if done:\n                episode_durations.append(t + 1)\n                plot_durations()\n                break\n        # Update the target network, copying all weights and biases in DQN\n        if i_episode % TARGET_UPDATE == 0:\n            target_net.load_state_dict(policy_net.state_dict())","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:22.242465Z","iopub.execute_input":"2021-10-24T08:28:22.243281Z","iopub.status.idle":"2021-10-24T08:28:22.254661Z","shell.execute_reply.started":"2021-10-24T08:28:22.243231Z","shell.execute_reply":"2021-10-24T08:28:22.253871Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 24, \"height\": 24, \"loglevel\": 2, \"annotations\": True}, debug=True)\nn_actions = 5\npolicy_net = LuxNet(n_actions)\ntarget_net = LuxNet(n_actions)\ntarget_net.load_state_dict(policy_net.state_dict())\ntarget_net.eval()\n\ntrain, val = train_test_split(samples, test_size=0.1, random_state=42, stratify=labels)\nbatch_size = 32\ntrain_loader = DataLoader(\n    LuxDataset(obses, train), \n    batch_size=batch_size, \n    shuffle=True, \n    num_workers=2\n)\nval_loader = DataLoader(\n    LuxDataset(obses, val), \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=2\n)\ndataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(target_net.parameters(), lr=1e-3)\n# optimizer = torch.optim.RMSprop(target_net.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:36.825107Z","iopub.execute_input":"2021-10-24T08:28:36.825367Z","iopub.status.idle":"2021-10-24T08:28:36.923320Z","shell.execute_reply.started":"2021-10-24T08:28:36.825338Z","shell.execute_reply":"2021-10-24T08:28:36.922495Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_model(target_net, dataloaders_dict, criterion, optimizer, num_epochs=15)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T07:07:02.177161Z","iopub.execute_input":"2021-10-24T07:07:02.177388Z","iopub.status.idle":"2021-10-24T07:07:07.532635Z","shell.execute_reply.started":"2021-10-24T07:07:02.177360Z","shell.execute_reply":"2021-10-24T07:07:07.531657Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"num_episodes = 50\nfor i_episode in range(num_episodes):\n    # Initialize the environment and state\n    env.reset()\n    last_screen = get_screen()\n    current_screen = get_screen()\n    state = current_screen - last_screen\n    for t in count():\n        # Select and perform an action\n        action = select_action(state)\n        _, reward, done, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n\n        # Observe new state\n        last_screen = current_screen\n        current_screen = get_screen()\n        if not done:\n            next_state = current_screen - last_screen\n        else:\n            next_state = None\n\n        # Store the transition in memory\n        memory.push(state, action, next_state, reward)\n\n        # Move to the next state\n        state = next_state\n\n        # Perform one step of the optimization (on the policy network)\n        optimize_model()\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n    # Update the target network, copying all weights and biases in DQN\n    if i_episode % TARGET_UPDATE == 0:\n        target_net.load_state_dict(policy_net.state_dict())","metadata":{"execution":{"iopub.status.busy":"2021-10-24T07:07:07.533741Z","iopub.status.idle":"2021-10-24T07:07:07.534764Z","shell.execute_reply.started":"2021-10-24T07:07:07.534512Z","shell.execute_reply":"2021-10-24T07:07:07.534536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"%%writefile agent.py\nimport os\nimport numpy as np\nimport torch\nfrom lux.game import Game\n\n\npath = '/kaggle_simulations/agent' if os.path.exists('/kaggle_simulations') else '.'\nmodel = torch.jit.load(f'{path}/model.pth')\nmodel.eval()\n\n\ndef make_input(obs, unit_id):\n    width, height = obs['width'], obs['height']\n    x_shift = (32 - width) // 2\n    y_shift = (32 - height) // 2\n    cities = {}\n    \n    b = np.zeros((20, 32, 32), dtype=np.float32)\n    \n    for update in obs['updates']:\n        strs = update.split(' ')\n        input_identifier = strs[0]\n        \n        if input_identifier == 'u':\n            x = int(strs[4]) + x_shift\n            y = int(strs[5]) + y_shift\n            wood = int(strs[7])\n            coal = int(strs[8])\n            uranium = int(strs[9])\n            if unit_id == strs[3]:\n                # Position and Cargo\n                b[:2, x, y] = (\n                    1,\n                    (wood + coal + uranium) / 100\n                )\n            else:\n                # Units\n                team = int(strs[2])\n                cooldown = float(strs[6])\n                idx = 2 + (team - obs['player']) % 2 * 3\n                b[idx:idx + 3, x, y] = (\n                    1,\n                    cooldown / 6,\n                    (wood + coal + uranium) / 100\n                )\n        elif input_identifier == 'ct':\n            # CityTiles\n            team = int(strs[1])\n            city_id = strs[2]\n            x = int(strs[3]) + x_shift\n            y = int(strs[4]) + y_shift\n            idx = 8 + (team - obs['player']) % 2 * 2\n            b[idx:idx + 2, x, y] = (\n                1,\n                cities[city_id]\n            )\n        elif input_identifier == 'r':\n            # Resources\n            r_type = strs[1]\n            x = int(strs[2]) + x_shift\n            y = int(strs[3]) + y_shift\n            amt = int(float(strs[4]))\n            b[{'wood': 12, 'coal': 13, 'uranium': 14}[r_type], x, y] = amt / 800\n        elif input_identifier == 'rp':\n            # Research Points\n            team = int(strs[1])\n            rp = int(strs[2])\n            b[15 + (team - obs['player']) % 2, :] = min(rp, 200) / 200\n        elif input_identifier == 'c':\n            # Cities\n            city_id = strs[2]\n            fuel = float(strs[3])\n            lightupkeep = float(strs[4])\n            cities[city_id] = min(fuel / lightupkeep, 10) / 10\n    \n    # Day/Night Cycle\n    b[17, :] = obs['step'] % 40 / 40\n    # Turns\n    b[18, :] = obs['step'] / 360\n    # Map Size\n    b[19, x_shift:32 - x_shift, y_shift:32 - y_shift] = 1\n\n    return b\n\n\ngame_state = None\ndef get_game_state(observation):\n    global game_state\n    \n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation[\"player\"]\n    else:\n        game_state._update(observation[\"updates\"])\n    return game_state\n\n\ndef in_city(pos):    \n    try:\n        city = game_state.map.get_cell_by_pos(pos).citytile\n        return city is not None and city.team == game_state.id\n    except:\n        return False\n\n\ndef call_func(obj, method, args=[]):\n    return getattr(obj, method)(*args)\n\n\nunit_actions = [('move', 'n'), ('move', 's'), ('move', 'w'), ('move', 'e'), ('build_city',)]\ndef get_action(policy, unit, dest):\n    for label in np.argsort(policy)[::-1]:\n        act = unit_actions[label]\n        pos = unit.pos.translate(act[-1], 1) or unit.pos\n        if pos not in dest or in_city(pos):\n            return call_func(unit, *act), pos \n            \n    return unit.move('c'), unit.pos\n\n\ndef agent(observation, configuration):\n    global game_state\n    \n    game_state = get_game_state(observation)    \n    player = game_state.players[observation.player]\n    actions = []\n    \n    # City Actions\n    unit_count = len(player.units)\n    for city in player.cities.values():\n        for city_tile in city.citytiles:\n            if city_tile.can_act():\n                if unit_count < player.city_tile_count: \n                    actions.append(city_tile.build_worker())\n                    unit_count += 1\n                elif not player.researched_uranium():\n                    actions.append(city_tile.research())\n                    player.research_points += 1\n    \n    # Worker Actions\n    dest = []\n    for unit in player.units:\n        if unit.can_act() and (game_state.turn % 40 < 30 or not in_city(unit.pos)):\n            state = make_input(observation, unit.id)\n            with torch.no_grad():\n                p = model(torch.from_numpy(state).unsqueeze(0))\n\n            policy = p.squeeze(0).numpy()\n\n            action, pos = get_action(policy, unit, dest)\n            actions.append(action)\n            dest.append(pos)\n\n    return actions","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:45.804810Z","iopub.execute_input":"2021-10-24T08:28:45.805082Z","iopub.status.idle":"2021-10-24T08:28:45.814377Z","shell.execute_reply.started":"2021-10-24T08:28:45.805053Z","shell.execute_reply":"2021-10-24T08:28:45.813647Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 24, \"height\": 24, \"loglevel\": 2, \"annotations\": True}, debug=False)\nsteps = env.run(['agent.py', 'agent.py'])\nenv.render(mode=\"ipython\", width=1200, height=800)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:46.030761Z","iopub.execute_input":"2021-10-24T08:28:46.031210Z","iopub.status.idle":"2021-10-24T08:28:46.108267Z","shell.execute_reply.started":"2021-10-24T08:28:46.031176Z","shell.execute_reply":"2021-10-24T08:28:46.107538Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"type(steps)\nprint( steps[2] )","metadata":{"execution":{"iopub.status.busy":"2021-10-24T08:28:52.937375Z","iopub.execute_input":"2021-10-24T08:28:52.937685Z","iopub.status.idle":"2021-10-24T08:28:52.956973Z","shell.execute_reply.started":"2021-10-24T08:28:52.937649Z","shell.execute_reply":"2021-10-24T08:28:52.955999Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"execution":{"iopub.status.busy":"2021-10-24T07:07:07.541480Z","iopub.status.idle":"2021-10-24T07:07:07.541903Z","shell.execute_reply.started":"2021-10-24T07:07:07.541665Z","shell.execute_reply":"2021-10-24T07:07:07.541687Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
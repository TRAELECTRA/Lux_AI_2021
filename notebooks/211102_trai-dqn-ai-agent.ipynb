{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kaggle-environments -U > /dev/null 2>&1s\n!cp -r ../input/lux-ai-2021/* .","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:16.404133Z","iopub.execute_input":"2021-11-02T13:50:16.404638Z","iopub.status.idle":"2021-11-02T13:50:17.865447Z","shell.execute_reply.started":"2021-11-02T13:50:16.404567Z","shell.execute_reply":"2021-11-02T13:50:17.864498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom pathlib import Path\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom itertools import count","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-02T13:50:30.576888Z","iopub.execute_input":"2021-11-02T13:50:30.577597Z","iopub.status.idle":"2021-11-02T13:50:35.641821Z","shell.execute_reply.started":"2021-11-02T13:50:30.577558Z","shell.execute_reply":"2021-11-02T13:50:35.641119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lux.constants import Constants\nfrom lux.game import Game\nfrom kaggle_environments import make","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.643258Z","iopub.execute_input":"2021-11-02T13:50:35.644194Z","iopub.status.idle":"2021-11-02T13:50:35.707888Z","shell.execute_reply.started":"2021-11-02T13:50:35.644148Z","shell.execute_reply":"2021-11-02T13:50:35.707162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"game_state = None\ndef get_game_state(observation):\n    global game_state\n    \n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation[\"player\"]\n    else:\n        game_state._update(observation[\"updates\"])\n    return game_state\n\n\ndef in_city(pos):    \n    try:\n        city = game_state.map.get_cell_by_pos(pos).citytile\n        return city is not None and city.team == game_state.id\n    except:\n        return False\n\n\ndef call_func(obj, method, args=[]):\n    return getattr(obj, method)(*args)\n\n\nunit_actions = [('move', 'n'), ('move', 's'), ('move', 'w'), ('move', 'e'), ('build_city',), ('move', 'c')]\ndef get_action(policy, unit, dest):\n    for label in np.argsort(policy)[::-1]:\n        act = unit_actions[label]\n        pos = unit.pos.translate(act[-1], 1) or unit.pos\n        if pos not in dest or in_city(pos):\n            return call_func(unit, *act), pos \n            \n    return unit.move('c'), unit.pos\n\n\ndef agent(observation, configuration):\n    global game_state, targetNet\n    model = CLuxNet(6)\n    game_state = get_game_state(observation)    \n    player = game_state.players[observation.player]\n    actions = []\n    \n    # City Actions\n    unit_count = len(player.units)\n    for city in player.cities.values():\n        for city_tile in city.citytiles:\n            if city_tile.can_act():\n                if unit_count < player.city_tile_count: \n                    actions.append(city_tile.build_worker())\n                    unit_count += 1\n                elif not player.researched_uranium():\n                    actions.append(city_tile.research())\n                    player.research_points += 1\n    \n    # Worker Actions\n    dest = []\n    model.load_state_dict(targetNet.state_dict())\n    for unit in player.units:\n        if unit.can_act() and (game_state.turn % 40 < 30 or not in_city(unit.pos)):\n            nStep: int = observation['step']\n            nXShift: int = (32 - observation['width']) // 2\n            nYShift: int = (32 - observation['height']) // 2\n            state = updateMap(nStep ,\\\n                              nXShift, \\\n                              nYShift, \\\n                              0, \\\n                              unit.id, \\\n                              observation['updates'])\n            with torch.no_grad():\n                p = model(torch.from_numpy(state).float().unsqueeze(0))\n\n            policy = p.squeeze(0).numpy()\n\n            action, pos = get_action(policy, unit, dest)\n            actions.append(action)\n            dest.append(pos)\n\n    return actions","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.70931Z","iopub.execute_input":"2021-11-02T13:50:35.709569Z","iopub.status.idle":"2021-11-02T13:50:35.727927Z","shell.execute_reply.started":"2021-11-02T13:50:35.709536Z","shell.execute_reply":"2021-11-02T13:50:35.727195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 0) Set Inputs","metadata":{}},{"cell_type":"code","source":"# Global Variables\n# < SYSTEM >\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAVE_PATH = './policy_network'\n\n# < TRAINING >\nEPS_START = 0.5\nEPS_END = 0.01\nEPS_DECAY = 200\nBATCH_SIZE = 32\nNUM_EPOCHS = 200\nGAMMA = 0.999\nSTEPS_DONE = 0\nREPLAY_CAPACITY = 1000\nTARGET_UPDATE = 10\nLEARNING_RATE = 1e-3\nBEST_ACCURACY = 0\n\n# < MAP >\nN_ACTIONS = 6\nMAX_WIDTH = 32\nMAX_HEIGHT = 32\nWIDTH = 24\nHEIGHT = 24","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.730035Z","iopub.execute_input":"2021-11-02T13:50:35.730375Z","iopub.status.idle":"2021-11-02T13:50:35.777506Z","shell.execute_reply.started":"2021-11-02T13:50:35.730338Z","shell.execute_reply":"2021-11-02T13:50:35.776754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"step[0] = [{'action': [], 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 60, 'step': 0, 'width': 24, 'height': 24, 'reward': 0, 'globalUnitIDCount': 2, 'globalCityIDCount': 2, 'player': 0, 'updates': ['0', '24 24', 'rp 0 0', 'rp 1 0', 'r uranium 0 0 310', 'r uranium 0 14 344', 'r uranium 0 23 304', 'r wood 3 10 333', 'r coal 3 23 407', 'r wood 4 7 323', 'r wood 4 8 324', 'r wood 4 10 318', 'r wood 4 11 360', 'r wood 5 8 345', 'r wood 5 9 303', 'r wood 5 10 351', 'r wood 5 21 800', 'r wood 5 22 800', 'r wood 6 8 387', 'r wood 6 9 342', 'r wood 6 10 326', 'r wood 6 11 398', 'r wood 6 12 393', 'r wood 6 19 350', 'r wood 6 20 331', 'r wood 6 21 800', 'r wood 7 10 355', 'r wood 7 11 376', 'r wood 7 12 310', 'r wood 7 19 364', 'r wood 7 20 396', 'r wood 8 11 346', 'r wood 8 12 394', 'r wood 8 20 385', 'r wood 9 10 376', 'r uranium 10 0 324', 'r uranium 10 1 322', 'r uranium 11 0 349', 'r coal 11 5 391', 'r coal 11 6 418', 'r uranium 12 0 349', 'r coal 12 5 391', 'r coal 12 6 418', 'r uranium 13 0 324', 'r uranium 13 1 322', 'r wood 14 10 376', 'r wood 15 11 346', 'r wood 15 12 394', 'r wood 15 20 385', 'r wood 16 10 355', 'r wood 16 11 376', 'r wood 16 12 310', 'r wood 16 19 364', 'r wood 16 20 396', 'r wood 17 8 387', 'r wood 17 9 342', 'r wood 17 10 326', 'r wood 17 11 398', 'r wood 17 12 393', 'r wood 17 19 350', 'r wood 17 20 331', 'r wood 17 21 800', 'r wood 18 8 345', 'r wood 18 9 303', 'r wood 18 10 351', 'r wood 18 21 800', 'r wood 18 22 800', 'r wood 19 7 323', 'r wood 19 8 324', 'r wood 19 10 318', 'r wood 19 11 360', 'r wood 20 10 333', 'r coal 20 23 407', 'r uranium 23 0 310', 'r uranium 23 14 344', 'r uranium 23 23 304', 'u 0 0 u_1 6 22 0 0 0 0', 'u 0 1 u_2 17 22 0 0 0 0', 'c 0 c_1 0 23', 'c 1 c_2 0 23', 'ct 0 c_1 6 22 0', 'ct 1 c_2 17 22 0', 'ccd 6 22 6', 'ccd 17 22 6', 'D_DONE']}, 'status': 'ACTIVE'}, {'action': [], 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 60, 'reward': 0, 'player': 1}, 'status': 'ACTIVE'}]\n\nstep[1] = [{'action': None, 'reward': None, 'info': {}, 'observation': {'remainingOverageTime': 60, 'step': 1, 'width': 24, 'height': 24, 'reward': 10001, 'globalUnitIDCount': 2, 'globalCityIDCount': 2, 'player': 0, 'updates': ['rp 0 0', 'rp 1 0', 'r uranium 0 0 310', 'r uranium 0 14 344', 'r uranium 0 23 304', 'r wood 3 10 342', 'r coal 3 23 407', 'r wood 4 7 332', 'r wood 4 8 333', 'r wood 4 10 326', 'r wood 4 11 369', 'r wood 5 8 354', 'r wood 5 9 311', 'r wood 5 10 360', 'r wood 5 21 800', 'r wood 5 22 780', 'r wood 6 8 397', 'r wood 6 9 351', 'r wood 6 10 335', 'r wood 6 11 408', 'r wood 6 12 403', 'r wood 6 19 359', 'r wood 6 20 340', 'r wood 6 21 780', 'r wood 7 10 364', 'r wood 7 11 386', 'r wood 7 12 318', 'r wood 7 19 374', 'r wood 7 20 406', 'r wood 8 11 355', 'r wood 8 12 404', 'r wood 8 20 395', 'r wood 9 10 386', 'r uranium 10 0 324', 'r uranium 10 1 322', 'r uranium 11 0 349', 'r coal 11 5 391', 'r coal 11 6 418', 'r uranium 12 0 349', 'r coal 12 5 391', 'r coal 12 6 418', 'r uranium 13 0 324', 'r uranium 13 1 322', 'r wood 14 10 386', 'r wood 15 11 355', 'r wood 15 12 404', 'r wood 15 20 395', 'r wood 16 10 364', 'r wood 16 11 386', 'r wood 16 12 318', 'r wood 16 19 374', 'r wood 16 20 406', 'r wood 17 8 397', 'r wood 17 9 351', 'r wood 17 10 335', 'r wood 17 11 408', 'r wood 17 12 403', 'r wood 17 19 359', 'r wood 17 20 340', 'r wood 17 21 780', 'r wood 18 8 354', 'r wood 18 9 311', 'r wood 18 10 360', 'r wood 18 21 800', 'r wood 18 22 780', 'r wood 19 7 332', 'r wood 19 8 333', 'r wood 19 10 326', 'r wood 19 11 369', 'r wood 20 10 342', 'r coal 20 23 407', 'r uranium 23 0 310', 'r uranium 23 14 344', 'r uranium 23 23 304', 'u 0 0 u_1 6 22 0 0 0 0', 'u 0 1 u_2 17 22 0 0 0 0', 'c 0 c_1 40 23', 'c 1 c_2 40 23', 'ct 0 c_1 6 22 0', 'ct 1 c_2 17 22 0', 'ccd 6 22 6', 'ccd 17 22 6', 'D_DONE']}, 'status': 'ERROR'}, {'action': None, 'reward': None, 'info': {}, 'observation': {'remainingOverageTime': 60, 'reward': 10001, 'player': 1}, 'status': 'ERROR'}]\n\nstep[100] = {'action': ['m u_1 w'],\n 'reward': 10001,\n 'info': {},\n 'observation': {'remainingOverageTime': 60,\n  'step': 100,\n  'width': 24,\n  'height': 24,\n  'reward': 10001,\n  'globalUnitIDCount': 2,\n  'globalCityIDCount': 2,\n  'player': 0,\n  'updates': ['rp 0 0',\n   'rp 1 0',\n   'r coal 0 3 367',\n   'r coal 0 4 389',\n   'r coal 0 8 410',\n   'r uranium 0 23 302',\n   'r wood 1 21 500',\n   'r wood 1 22 500',\n   'r wood 1 23 500',\n   'r wood 2 21 500',\n   'r wood 2 22 500',\n   'r wood 3 22 500',\n   'r wood 6 11 500',\n   'r wood 7 10 500',\n   'r wood 8 1 500',\n   'r wood 8 2 500',\n   'r wood 8 3 500',\n   'r wood 8 9 500',\n   'r wood 8 10 500',\n   'r uranium 8 16 328',\n   'r uranium 8 17 310',\n   'r coal 8 22 354',\n   'r coal 8 23 396',\n   'r wood 9 9 500',\n   'r wood 9 10 500',\n   'r uranium 9 17 342',\n   'r coal 9 23 394',\n   'r wood 10 10 500',\n   'r wood 13 10 500',\n   'r wood 14 9 500',\n   'r wood 14 10 500',\n   'r uranium 14 17 342',\n   'r coal 14 23 394',\n   'r wood 15 1 500',\n   'r wood 15 2 500',\n   'r wood 15 3 500',\n   'r wood 15 9 500',\n   'r wood 15 10 500',\n   'r uranium 15 16 328',\n   'r uranium 15 17 310',\n   'r coal 15 22 354',\n   'r coal 15 23 396',\n   'r wood 16 10 500',\n   'r wood 17 11 500',\n   'r wood 20 22 500',\n   'r wood 21 21 500',\n   'r wood 21 22 500',\n   'r wood 22 21 500',\n   'r wood 22 22 500',\n   'r wood 22 23 500',\n   'r coal 23 3 367',\n   'r coal 23 4 389',\n   'r coal 23 8 410',\n   'r uranium 23 23 302',\n   'u 0 0 u_1 7 2 0 0 0 0',\n   'u 0 1 u_2 16 2 0 0 0 0',\n   'c 0 c_1 2240 23',\n   'c 1 c_2 2240 23',\n   'ct 0 c_1 7 2 0',\n   'ct 1 c_2 16 2 0',\n   'ccd 7 2 6',\n   'ccd 16 2 6',\n   'D_DONE']},\n 'status': 'ACTIVE'}","metadata":{}},{"cell_type":"code","source":"INPUT_CONSTANTS = Constants.INPUT_CONSTANTS\nRESOURCE_TYPES = Constants.RESOURCE_TYPES\n\ndef updateMap(nStep: int, \\\n              nXShift: int, \\\n              nYShift: int, \\\n              nTeam: int, \\\n              sUId: str, \\\n              updateList: list) -> list:\n\n    # indexing\n    # rp  - gameMap[0:2]                  #resource points\n    # r   - gameMap[2:5]                  #resource\n    # u   - gameMap[5:13]                 #unit\n    # c   - ...it only consumes fuels     #city\n    # ct  - gameMap[8:12]                 #citytile\n    # ccd - gameMap[]                     #roads (city cool down)\n\n    rpStart = 0\n    rStart = 2\n    uStart = 5\n    ctStart = 8\n\n    gameMap = np.zeros((20, MAX_WIDTH, MAX_HEIGHT))\n    cityDict: dict = {}\n\n    for update in updateList:\n        cmdList: list[str] = update.split(' ')\n\n        sIdentifier: str = cmdList[0]\n        if INPUT_CONSTANTS.RESEARCH_POINTS == sIdentifier:\n            team = int(cmdList[1])\n            rp = int(cmdList[2])\n            idx = rpStart + (team - nTeam) % 2\n            value = min(rp, 200) / 200\n            gameMap[idx, :] = value\n\n        elif INPUT_CONSTANTS.RESOURCES == sIdentifier:\n            rtype = cmdList[1]\n            x = int(cmdList[2]) + nXShift\n            y = int(cmdList[3]) + nYShift\n            amt = int(float(cmdList[4]))\n            idx = rStart + {'wood':0, 'coal':1, 'uranium':2}[rtype]\n            value = amt / 800\n            gameMap[idx, x, y] = value\n\n        elif INPUT_CONSTANTS.UNITS == sIdentifier:\n            utype = int(cmdList[1])\n            team = int(cmdList[2])\n            uid = cmdList[3]\n            x = int(cmdList[4])\n            y = int(cmdList[5])\n            cooldown = float(cmdList[6]) / 6.0\n            wood = int(cmdList[7])\n            coal = int(cmdList[8])\n            uranium = int(cmdList[9])\n            resources = (wood + coal + uranium) / 100\n\n            if sUId == uid:\n                idx = uStart\n                value = (1, resources)\n                gameMap[idx:idx+2, x, y] = value\n            else:\n                idx = uStart + 2\n                value = (1, cooldown, resources)\n                gameMap[idx:idx+3, x, y] = value\n\n        elif INPUT_CONSTANTS.CITY == sIdentifier:\n            team = int(cmdList[1])\n            cid: str = cmdList[2]\n            fuel = float(cmdList[3])\n            lightupkeep = float(cmdList[4])\n            cityDict[cid] = min(fuel / lightupkeep, 10) / 10\n\n        elif INPUT_CONSTANTS.CITY_TILES == sIdentifier:\n            team = int(cmdList[1])\n            cid: str = cmdList[2]\n            x = int(cmdList[3]) + nXShift\n            y = int(cmdList[4]) + nYShift\n            cooldown = float(cmdList[5])\n            idx = ctStart + (team - nTeam) % 2 * 2\n            value = (1, cityDict[cid])\n            gameMap[idx:idx+2, x, y] = value\n\n        elif INPUT_CONSTANTS.ROADS == sIdentifier:\n            x = int(cmdList[1])\n            y = int(cmdList[2])\n            road = float(cmdList[3])\n\n\n    # Day/Night Cycle\n    gameMap[17, :] = nStep % 40 / 40\n    # Turns\n    gameMap[18, :] = nStep / 360\n    # Map Size\n    gameMap[19, nXShift:MAX_WIDTH-nXShift, nYShift:MAX_HEIGHT-nYShift] = 1\n\n    return gameMap\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.780745Z","iopub.execute_input":"2021-11-02T13:50:35.781016Z","iopub.status.idle":"2021-11-02T13:50:35.802863Z","shell.execute_reply.started":"2021-11-02T13:50:35.780987Z","shell.execute_reply":"2021-11-02T13:50:35.802081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def toLabel(player, action):\n    if action is None or len(action) < 1:\n        return f'u_{player}', 5\n    strs = action[0].split(' ')\n    unit_id = strs[1]\n    if strs[0] == 'm':\n        label = {'c': 5, 'n': 0, 's': 1, 'w': 2, 'e': 3}[strs[2]]\n    elif strs[0] == 'bcity':\n        label = 4\n    else:\n        label = 5\n    return unit_id, label\n\ndef depletedResources(obs):\n    for u in obs['updates']:\n        if u.split(' ')[0] == 'r':\n            return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.805929Z","iopub.execute_input":"2021-11-02T13:50:35.806214Z","iopub.status.idle":"2021-11-02T13:50:35.816088Z","shell.execute_reply.started":"2021-11-02T13:50:35.8061Z","shell.execute_reply":"2021-11-02T13:50:35.815307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1) Network","metadata":{}},{"cell_type":"code","source":"class CBasicConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            input_dim, output_dim, \n            kernel_size=kernel_size, \n            padding=(kernel_size[0] // 2, kernel_size[1] // 2)\n        )\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = self.conv(x)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\nclass CLuxNet(nn.Module):\n    def __init__( self, nActions ):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv = CBasicConv2d(20, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([\n            CBasicConv2d(filters, filters, (3, 3), True) for _ in range( layers )\n        ])\n        self.head = nn.Linear( filters, nActions, bias=False )\n\n    def forward(self, x):\n        h = F.relu_( self.conv(x) )\n        for b in self.blocks:\n            h = F.relu_( h + b( h ) )\n        h = ( h * x[:, :1] ).view( h.size(0), h.size(1), -1 ).sum(-1)\n        ret = self.head( h )\n        return ret","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.8174Z","iopub.execute_input":"2021-11-02T13:50:35.81779Z","iopub.status.idle":"2021-11-02T13:50:35.832582Z","shell.execute_reply.started":"2021-11-02T13:50:35.817731Z","shell.execute_reply":"2021-11-02T13:50:35.83169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2) Replay Memory","metadata":{}},{"cell_type":"code","source":"# Input for ReplayMemory\nfrom collections import namedtuple, deque\nData = namedtuple('Data',\n                  ('state', 'action', 'next_state', 'reward'))\n\n# state: list(str) = state\n# action: list(str) = step[0]['action']\n# next_state: list(str) = step[0]['observation']['updates']\nclass CReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([],maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a Data\"\"\"\n        self.memory.append(Data(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n    \nmemory = CReplayMemory(REPLAY_CAPACITY)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.835029Z","iopub.execute_input":"2021-11-02T13:50:35.835628Z","iopub.status.idle":"2021-11-02T13:50:35.846431Z","shell.execute_reply.started":"2021-11-02T13:50:35.835591Z","shell.execute_reply":"2021-11-02T13:50:35.845469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3) Select Action w/ noise","metadata":{}},{"cell_type":"code","source":"# NO NEED....\ndef select_action(state, model: CLuxNet):\n    global debug\n    sample = random.random() # check - range?\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * STEPS_DONE / EPS_DECAY)\n    STEPS_DONE += 1\n    if sample > eps_threshold:\n        with torch.no_grad():\n            # t.max(1) will return largest column value of each row.\n            # second column on max result is index of where max element was\n            # found, so we pick action with the larger expected reward.\n            return model(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(n_actions)]], device=DEVICE, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.848154Z","iopub.execute_input":"2021-11-02T13:50:35.848593Z","iopub.status.idle":"2021-11-02T13:50:35.858099Z","shell.execute_reply.started":"2021-11-02T13:50:35.84855Z","shell.execute_reply":"2021-11-02T13:50:35.857134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4) Optimize Model","metadata":{}},{"cell_type":"code","source":"def optimizeModel(memory: CReplayMemory, \\\n                  policyNet: CLuxNet, \\\n                  targetNet: CLuxNet, \\\n                  optimizer) -> None:\n    global BEST_ACCURACY\n    # -1) return exceptions\n    if STEPS_DONE == 0 or STEPS_DONE % BATCH_SIZE != 0 or len(memory) < BATCH_SIZE:\n        return\n    \n    # 1) fetch memory in batch size\n    datas = memory.sample(BATCH_SIZE)\n\n    # 2) make in a bulk list of Data type\n    datas = Data(*zip(*datas))\n    \n    # 3) concatenate state, action, reward\n    states = torch.cat([torch.tensor(s) for s in datas.state])\n    actions = torch.cat([torch.tensor(a) for a in np.array(datas.action).reshape(1, -1)])\n    rewards = torch.cat([torch.tensor(r) for r in datas.reward])\n    \n    # 4) next state mask\n    nextStateMask = torch.tensor(\n        tuple(map(lambda n_s: n_s is not None, datas.next_state)), \\\n        device = DEVICE, \\\n        dtype = torch.bool\n    )\n    \n    # 5) concatenate next state\n    nextStates = torch.cat([\n        torch.tensor(n_s) for n_s in datas.next_state if n_s is not None\n    ])\n    \n    # ** RESIZE INPUTS & SELECT DEVICE\n    states = states.view(BATCH_SIZE, -1, MAX_WIDTH, MAX_HEIGHT).float().to(DEVICE)\n    nextStates = nextStates.view(BATCH_SIZE, -1, MAX_WIDTH, MAX_HEIGHT).float().to(DEVICE)    \n    actions = actions.view(1, -1).to(DEVICE)\n    rewards.to(DEVICE)\n    \n    # 6) Compute Q-Value( Q(s_t, a) ), and select the columns of actions taken for each batch size\n    qValue = policyNet(states).gather(1, actions)\n    \n    # 7) Compute V(s_{t+1}) for all next states\n    vValue = torch.zeros(BATCH_SIZE, device=DEVICE)\n    vValue[nextStateMask] = targetNet(nextStates).max(1)[0].detach() #select action 가능\n    \n    # 8) Compute expected Q-Values with discount rate\n    expcQValue = (vValue * GAMMA + rewards).unsqueeze(1)\n    \n    # 9) Compute Huber Loss\n    criterion = nn.SmoothL1Loss()\n    loss = criterion(qValue.view(-1, 1), expcQValue)\n    acc = torch.sum(vValue[nextStateMask] == actions.data.squeeze(0)[nextStateMask]) / len(vValue[nextStateMask])\n    \n    # 10) Save the best model\n    if BEST_ACCURACY < acc:\n        BEST_ACCURACY = acc\n        torch.save(targetNet, SAVE_PATH)\n        \n    # 11) Log\n    print(f':: {STEPS_DONE} STEP :: Acc({acc}/{BEST_ACCURACY}), loss({loss}), QValue({expcQValue.min()}, {expcQValue.max()})')\n    \n    # 12) Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policyNet.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.861688Z","iopub.execute_input":"2021-11-02T13:50:35.862206Z","iopub.status.idle":"2021-11-02T13:50:35.880529Z","shell.execute_reply.started":"2021-11-02T13:50:35.862172Z","shell.execute_reply":"2021-11-02T13:50:35.879699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5) Trainig","metadata":{}},{"cell_type":"code","source":"unit_actions = [('move', 'n'), ('move', 's'), ('move', 'w'), ('move', 'e'), ('build_city',), ('move', 'c')]\ndef get_action(policy, unit, dest):\n    for label in np.argsort(policy)[::-1]:\n        act = unit_actions[label]\n        pos = unit.pos.translate(act[-1], 1) or unit.pos\n        if pos not in dest or in_city(pos):\n            return call_func(unit, *act), pos \n            \n    return unit.move('c'), unit.pos\n","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.883614Z","iopub.execute_input":"2021-11-02T13:50:35.883838Z","iopub.status.idle":"2021-11-02T13:50:35.89391Z","shell.execute_reply.started":"2021-11-02T13:50:35.883813Z","shell.execute_reply":"2021-11-02T13:50:35.892947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainModel(policyNet: CLuxNet, \\\n               targetNet: CLuxNet, \\\n               optimizer, \\\n               width: int, \\\n               height: int) -> None:\n    global STEPS_DONE\n        \n    targetNet.cuda()\n    policyNet.cuda()\n    \n    targetNet.eval()\n    policyNet.train()\n        \n    for epoch in range(NUM_EPOCHS):\n        env = make(\"lux_ai_2021\", configuration={\"width\": WIDTH, \"height\": HEIGHT, \"loglevel\": 2, \"annotations\": True}, debug=True)\n        steps = env.run([agent, agent])\n        \n        xShift, yShift = 0, 0\n        nextState = torch.zeros((20, MAX_WIDTH, MAX_HEIGHT), dtype=torch.float32)\n        for s in steps:\n            if len(s) > 1:\n                step = s[0]\n            else:\n                step = s\n            \n            observation = step['observation']\n            \n            depletedResources(observation)\n            \n            nStep: int = observation['step']\n            nTeam: int = observation['player']\n            sUId, action = toLabel(nTeam, step['action'])\n            \n            if nStep == 0:\n                width, height = observation['width'], observation['height']\n                xShift, yShift = (MAX_WIDTH - width) // 2, (MAX_HEIGHT - height) // 2\n                    \n            state = nextState\n            reward = torch.tensor([step['reward'] if step['reward'] is not None else 5], device=DEVICE)\n            nextState = updateMap(nStep, xShift, yShift, nTeam, sUId, observation['updates'])\n            \n            memory.push(state, action, nextState, reward)\n            optimizeModel(memory, policyNet, targetNet, optimizer)\n            \n            STEPS_DONE += 1\n        \n        if epoch % TARGET_UPDATE == 0:\n            targetNet.load_state_dict(policyNet.state_dict())\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.897282Z","iopub.execute_input":"2021-11-02T13:50:35.897528Z","iopub.status.idle":"2021-11-02T13:50:35.911321Z","shell.execute_reply.started":"2021-11-02T13:50:35.897491Z","shell.execute_reply":"2021-11-02T13:50:35.910474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6) Run","metadata":{}},{"cell_type":"code","source":"policyNet = CLuxNet(N_ACTIONS).to(DEVICE)\ntargetNet = CLuxNet(N_ACTIONS).to(DEVICE)\ntargetNet.load_state_dict(policyNet.state_dict())\noptimizer = torch.optim.AdamW(targetNet.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:35.912981Z","iopub.execute_input":"2021-11-02T13:50:35.913343Z","iopub.status.idle":"2021-11-02T13:50:41.08911Z","shell.execute_reply.started":"2021-11-02T13:50:35.913303Z","shell.execute_reply":"2021-11-02T13:50:41.088365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainModel(policyNet, targetNet, optimizer, WIDTH, HEIGHT)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T13:50:41.090451Z","iopub.execute_input":"2021-11-02T13:50:41.090717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"%%writefile agent.py\nimport os\nimport numpy as np\nimport torch\nfrom lux.game import Game\n\n\npath = '/kaggle_simulations/agent' if os.path.exists('/kaggle_simulations') else '.'\nmodel = torch.jit.load(f'{path}/model.pth')\nmodel.eval()\n\n\ndef make_input(obs, unit_id):\n    width, height = obs['width'], obs['height']\n    x_shift = (32 - width) // 2\n    y_shift = (32 - height) // 2\n    cities = {}\n    \n    b = np.zeros((20, 32, 32), dtype=np.float32)\n    \n    for update in obs['updates']:\n        strs = update.split(' ')\n        input_identifier = strs[0]\n        \n        if input_identifier == 'u':\n            x = int(strs[4]) + x_shift\n            y = int(strs[5]) + y_shift\n            wood = int(strs[7])\n            coal = int(strs[8])\n            uranium = int(strs[9])\n            if unit_id == strs[3]:\n                # Position and Cargo\n                b[:2, x, y] = (\n                    1,\n                    (wood + coal + uranium) / 100\n                )\n            else:\n                # Units\n                team = int(strs[2])\n                cooldown = float(strs[6])\n                idx = 2 + (team - obs['player']) % 2 * 3\n                b[idx:idx + 3, x, y] = (\n                    1,\n                    cooldown / 6,\n                    (wood + coal + uranium) / 100\n                )\n        elif input_identifier == 'ct':\n            # CityTiles\n            team = int(strs[1])\n            city_id = strs[2]\n            x = int(strs[3]) + x_shift\n            y = int(strs[4]) + y_shift\n            idx = 8 + (team - obs['player']) % 2 * 2\n            b[idx:idx + 2, x, y] = (\n                1,\n                cities[city_id]\n            )\n        elif input_identifier == 'r':\n            # Resources\n            r_type = strs[1]\n            x = int(strs[2]) + x_shift\n            y = int(strs[3]) + y_shift\n            amt = int(float(strs[4]))\n            b[{'wood': 12, 'coal': 13, 'uranium': 14}[r_type], x, y] = amt / 800\n        elif input_identifier == 'rp':\n            # Research Points\n            team = int(strs[1])\n            rp = int(strs[2])\n            b[15 + (team - obs['player']) % 2, :] = min(rp, 200) / 200\n        elif input_identifier == 'c':\n            # Cities\n            city_id = strs[2]\n            fuel = float(strs[3])\n            lightupkeep = float(strs[4])\n            cities[city_id] = min(fuel / lightupkeep, 10) / 10\n    \n    # Day/Night Cycle\n    b[17, :] = obs['step'] % 40 / 40\n    # Turns\n    b[18, :] = obs['step'] / 360\n    # Map Size\n    b[19, x_shift:32 - x_shift, y_shift:32 - y_shift] = 1\n\n    return b\n\n\ngame_state = None\ndef get_game_state(observation):\n    global game_state\n    \n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation[\"player\"]\n    else:\n        game_state._update(observation[\"updates\"])\n    return game_state\n\n\ndef in_city(pos):    \n    try:\n        city = game_state.map.get_cell_by_pos(pos).citytile\n        return city is not None and city.team == game_state.id\n    except:\n        return False\n\n\ndef call_func(obj, method, args=[]):\n    return getattr(obj, method)(*args)\n\n\nunit_actions = [('move', 'n'), ('move', 's'), ('move', 'w'), ('move', 'e'), ('build_city',)]\ndef get_action(policy, unit, dest):\n    for label in np.argsort(policy)[::-1]:\n        act = unit_actions[label]\n        pos = unit.pos.translate(act[-1], 1) or unit.pos\n        if pos not in dest or in_city(pos):\n            return call_func(unit, *act), pos \n            \n    return unit.move('c'), unit.pos\n\n\ndef agent(observation, configuration):\n    global game_state\n    \n    game_state = get_game_state(observation)    \n    player = game_state.players[observation.player]\n    actions = []\n    \n    # City Actions\n    unit_count = len(player.units)\n    for city in player.cities.values():\n        for city_tile in city.citytiles:\n            if city_tile.can_act():\n                if unit_count < player.city_tile_count: \n                    actions.append(city_tile.build_worker())\n                    unit_count += 1\n                elif not player.researched_uranium():\n                    actions.append(city_tile.research())\n                    player.research_points += 1\n    \n    # Worker Actions\n    dest = []\n    for unit in player.units:\n        if unit.can_act() and (game_state.turn % 40 < 30 or not in_city(unit.pos)):\n            state = make_input(observation, unit.id)\n            with torch.no_grad():\n                p = model(torch.from_numpy(state).unsqueeze(0))\n\n            policy = p.squeeze(0).numpy()\n\n            action, pos = get_action(policy, unit, dest)\n            actions.append(action)\n            dest.append(pos)\n\n    return actions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 24, \"height\": 24, \"loglevel\": 2, \"annotations\": True}, debug=False)\nsteps = env.run([agent, agent])\nenv.render(mode=\"ipython\", width=1200, height=800)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}